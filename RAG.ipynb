{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "transformers==4.31.0 \\\n",
    "sentence-transformers==2.2.2 \\\n",
    "pinecone-client==2.2.2 \\\n",
    "datasets==2.14.0 \\\n",
    "accelerate==0.21.0 \\\n",
    "einops==0.6.1 \\\n",
    "langchain==0.0.240 \\\n",
    "xformers==0.0.20 \\\n",
    "bitsandbytes==0.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Hugginface Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988d09eb40964ee6b8dbf6dc9281abd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12af7bfceb5e4505845eee5bd1ac0d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a89aad17fb4a0f88edccf263b9acc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee47619021e4a2997bd89f8db44555e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9014d010f7427ea57a5fa7690d5049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7e541ef8bc451783b68b72fcb274c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c86750e1bf0433b8fcca1df1ba6be17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cc9b11aaab458bb386d6bf66d909ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e77c2630a84b7a9e6cc368cea386af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32afc9382ba14ec99609936adfa1f5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b16367b7c34a0583cdbfb31487e3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de35d71b8dd94782ac44ee1cf4ddc14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a862b1260d44dc9cbf6c5ebfbe1b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a3a341f547448aa5b5d0b28e3da853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#The embedding\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "  model_name=embed_model_id,\n",
    "  model_kwargs={'device': device},\n",
    "  encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Docs\n",
    "\n",
    "Each doc in this project is a string containing sociodemographic features o an anonimous person, a question and the answer of the person described. So each text is like the following example:\n",
    "\n",
    "The CEP National Public Opinion Survey is an academic analysis of the political, economic and social attitudes and perceptions of the Chilean population. Below is a profile with the data of one of the people surveyed last year 2023, the question asked and the answer given. Person's features: Person 24.0 years old, female. She lives in the Metropolitan region in Chile, in an urban area. Her socioeconomic level is C3 and her educational level is Complete Technical Institute. Regarding religion, she considers herself agnostic. Regarding politics, on the left-right scale she feels closer to the left, the party with which she most sympathizes is the Socialist Party (PS). Question: Next, I am going to read you the name of an institution. According to the card alternatives, how much trust do you have in the institution that I name you below? Policies. Alternatives: A lot of confidence, A lot of confidence, Little confidence, No confidence. Answer: Little trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charge a json with the following keys: \"prompt\", \"questions\",\"alternatives\",\"answers\", \"year\"\n",
    "\n",
    "#Docs will have the following structure:\n",
    "\"\"\" docs = [\n",
    "    \"this is one document\",\n",
    "    \"and another document\"\n",
    "] \"\"\"\n",
    "\n",
    "json_name = \"translated_data_89_pres.json\"\n",
    "with open(json_name, 'r', encoding='utf-8') as f:\n",
    "  df_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the dataset is cleaned from undesirable data. In particular, alternatives are presented in the questions, and also in the alternatives keys.\n",
    "\n",
    "The alternatives from the questions will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_point in df_json:\n",
    "  alternatives = data_point[\"alternatives\"]\n",
    "  question = data_point[\"question\"]\n",
    "\n",
    "  data_point[\"question\"] = question.replace(alternatives, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point, year):\n",
    "  cep = f\"\"\"La Encuesta Nacional de Opinión Pública CEP es un análisis académico de las actitudes y \\\n",
    "  percepciones políticas, económicas y sociales de la población chilena. \\\n",
    "  A continuación se presenta un perfil con los datos de una de las personas\\\n",
    "   encuestadas el año {year}, indicando datos sociodemográficos, pregunta que se le hizo a dicha persona y su respuesta.\"\"\"\n",
    "\n",
    "  cep_eng = f\"\"\"The CEP National Public Opinion Survey is an academic analysis \\\n",
    "    of the political, economic and social attitudes and perceptions of the Chilean population. \\\n",
    "      Below is a profile with the data of one of the people surveyed the year {year}, \\\n",
    "        the question asked and the answer given.\"\"\"\n",
    "  if english:\n",
    "    prompt = f\"\"\"{cep_eng} Person's features: {data_point[\"prompt\"]}. \\\n",
    "    Question: {data_point[\"question\"]}. \\\n",
    "    Alternatives: [ {data_point[\"alternatives\"]}. \\\n",
    "    Answers: {data_point[\"answers\"]}\"\"\"\n",
    "  \n",
    "  else:\n",
    "    prompt =  f\"\"\"{cep} Características de la persona: {data_point[\"prompt\"]}.\\\n",
    "     Pregunta: {data_point[\"question\"]}. Alternativas: {data_point[\"alternatives\"]}\\\n",
    "      Respuesta: {data_point[\"answers\"]}\"\"\"\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating docs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' with open(\\'train_\\' + str(n_samples) + \\'_ONLY_PRES_ENG.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n  print(\"Saving json\")\\n  json.dump(datos_modificados, f, ensure_ascii=False, indent=4) '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs = []\n",
    "print(\"generating docs...\")\n",
    "i = 0\n",
    "for entrada in df_json:\n",
    "\n",
    "  # Combinar el texto y la pregunta\n",
    "  text = generate_prompt(entrada, 2023)\n",
    "  # Crear un nuevo diccionario con el formato deseado\n",
    "  docs.append(text)\n",
    "\n",
    "# Escribir los datos modificados en un nuevo archivo JSON\n",
    "#print(datos_modificados)\n",
    "\"\"\" with open('train_' + str(n_samples) + '_ONLY_PRES_ENG.json', 'w', encoding='utf-8') as f:\n",
    "  print(\"Saving json\")\n",
    "  json.dump(datos_modificados, f, ensure_ascii=False, indent=4) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1181 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "  f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or '*******************************',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or '**********'\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '*******************************'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "  model_id,\n",
    "  use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "  model=model, tokenizer=tokenizer,\n",
    "  return_full_text=True,  # langchain expects the full text\n",
    "  task='text-generation',\n",
    "  # we pass model parameters here too\n",
    "  temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "  max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "  repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('what is so special about llama 2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline('what is so special about llama 2?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
